{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMsXUOc66GuEzgqgazipW5s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ranwiththecode/ficnest_webapp/blob/main/metadata_addition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBKPy_IY0ox_",
        "outputId": "3c616f27-3193-4aa5-c9e9-fa66e18862ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "True\n",
            "True\n",
            "✅ Summary updated for: Fools Errand\n",
            "Discarded %:0.88%\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from google.cloud import bigquery\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "def summarize_preprocessing(cleaned_file=\"/content/drive/MyDrive/Goodreads_Data/fools_errand_clean.csv\",\n",
        "                            original_file=\"/content/drive/MyDrive/Goodreads_Data/fools_errand_reviews.csv\",\n",
        "                            summary_file=\"/content/drive/MyDrive/Goodreads_Data/processing_summary.csv\",\n",
        "                            project_id=\"stable-course-461105-k9\",\n",
        "                            dataset=\"google_imports\",\n",
        "                            table=\"pubs_2000_2005\"):\n",
        "\n",
        "    print(os.path.exists(cleaned_file))\n",
        "    print(os.path.exists(original_file))\n",
        "\n",
        "    \"\"\"\n",
        "    Summarizes preprocessing results for a single book and appends metadata from BigQuery.\n",
        "\n",
        "    Args:\n",
        "        cleaned_file (str): Path to the cleaned review CSV file.\n",
        "        original_file (str): Path to the original review CSV file.\n",
        "        summary_file (str): Path to the summary CSV to update or create.\n",
        "        project_id (str): GCP project ID.\n",
        "        dataset (str): BigQuery dataset name.\n",
        "        table (str): BigQuery table name.\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract book title from cleaned filename\n",
        "    book_title = os.path.basename(cleaned_file).replace(\"_clean.csv\", \"\").replace(\"_\", \" \").title()\n",
        "\n",
        "    # Load cleaned and original data\n",
        "    try:\n",
        "        cleaned_df = pd.read_csv(cleaned_file)\n",
        "        original_df = pd.read_csv(original_file)\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading files for {book_title}: {e}\")\n",
        "        return\n",
        "\n",
        "    filtered_count = len(cleaned_df)\n",
        "    original_count = len(original_df)\n",
        "\n",
        "    # Query metadata from BigQuery\n",
        "    client = bigquery.Client(project=project_id)\n",
        "    query = f\"\"\"\n",
        "    SELECT `Author Pronouns` AS author_gender, Protagonist AS protagonist_gender, `Publication Year` AS pub_year\n",
        "    FROM `{project_id}.{dataset}.{table}`\n",
        "    WHERE LOWER(Name) = @book_title\n",
        "    LIMIT 1\n",
        "    \"\"\"\n",
        "    job_config = bigquery.QueryJobConfig(\n",
        "        query_parameters=[\n",
        "            bigquery.ScalarQueryParameter(\"book_title\", \"STRING\", book_title.lower())\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        results = client.query(query, job_config=job_config).result()\n",
        "        row = next(results, None)\n",
        "    except Exception as e:\n",
        "        print(f\"BigQuery error for {book_title}: {e}\")\n",
        "        row = None\n",
        "\n",
        "    author_gender = row.author_gender if row else \"Unknown\"\n",
        "    protagonist_gender = row.protagonist_gender if row else \"Unknown\"\n",
        "    pub_year = row.pub_year if row and row.pub_year is not None else \"Unknown\"\n",
        "\n",
        "    # Build summary row\n",
        "    summary_data = {\n",
        "        \"Book Title\": book_title,\n",
        "        \"Original Reviews\": original_count,\n",
        "        \"Filtered Reviews\": filtered_count,\n",
        "        \"Discarded Reviews\": original_count - filtered_count,\n",
        "        \"Discarded %\": f\"{(original_count - filtered_count) / original_count:.2%}\",\n",
        "        \"Author Gender\": author_gender,\n",
        "        \"Protagonist Gender\": protagonist_gender,\n",
        "        \"Pub Year\": pub_year\n",
        "    }\n",
        "\n",
        "    summary_df = pd.DataFrame([summary_data])\n",
        "\n",
        "\n",
        "   # Append or update summary file\n",
        "    if os.path.exists(summary_file):\n",
        "      existing_df = pd.read_csv(summary_file)\n",
        "\n",
        "      # Check if book already exists in the summary\n",
        "      existing_df[\"Book Title\"] = existing_df[\"Book Title\"].astype(str)\n",
        "      match = existing_df[\"Book Title\"].str.lower() == book_title.lower()\n",
        "\n",
        "      if match.any():\n",
        "        # Get index of the matching row\n",
        "        row_index = existing_df[match].index[0]\n",
        "\n",
        "        # Overwrite that row with new summary data\n",
        "        for col in summary_df.columns:\n",
        "            existing_df.at[row_index, col] = summary_df.iloc[0][col]\n",
        "\n",
        "        updated_df = existing_df\n",
        "      else:\n",
        "          updated_df = pd.concat([existing_df, summary_df], ignore_index=True)\n",
        "\n",
        "    else:\n",
        "        updated_df = summary_df\n",
        "\n",
        "\n",
        "    updated_df.to_csv(summary_file, index=False)\n",
        "    print(f\"✅ Summary updated for: {book_title}\")\n",
        "    print(f\"Discarded %:{(original_count - filtered_count) / original_count:.2%}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    summarize_preprocessing()\n"
      ]
    }
  ]
}